# Обучение модели hr search 

Этот репозиторий содержит код для обучения модели поиска с использованием модели paraphrase-multilingual-mpnet-base-v2 из библиотеки Hugging Face Transformers.

Для обучения retrieval sentence трансформера, который использует смещение эмбеддингов ближайших по косинусной близости, используется метод обучения с учителем (supervised learning), где модель обучается на основе известных пар "запрос-ответ" или "запрос-документ". Основная идея состоит в том, чтобы модель выучила представление векторов таким образом, чтобы семантически похожие предложения были близки в векторном пространстве, а несхожие — далеко друг от друга.

Процесс обучения можно описать следующими шагами:

### 1. Подготовка обучающего датасета
Датасет должен содержать пары или тройки текстов, которые могут быть положительными примерами (семантически похожие тексты) и отрицательными примерами (семантически различные тексты). Например, для задачи поиска ответов на вопросы (Question Answering) каждая тренировочная запись может содержать вопрос, правильный ответ и несколько неправильных ответов.

### 2. Вычисление эмбеддингов
Для каждого текста в паре или тройке вычисляется эмбеддинг с использованием предварительно обученного трансформера (например, BERT, RoBERTa, DistilBERT и т.д.).

### 3. Метрика потерь (Loss Function)
Используется метрика потерь, которая учитывает косинусную близость между эмбеддингами. Одной из распространенных функций потерь является триплетная потеря (Triplet Loss), где модель стремится сделать расстояние между положительными примерами (anchor и positive) как можно меньше, а между anchor и negative — как можно больше.

### 4. Смещение эмбеддингов
Во время обучения, после вычисления эмбеддингов, можно использовать технику смешения, когда эмбеддинг для запроса смешивается с эмбеддингами ближайших положительных и отрицательных примеров, чтобы улучшить обобщающую способность модели. Это может быть сделано, например, путем усреднения эмбеддингов или использования взвешенной суммы.

### 5. Оптимизация
Используется оптимизатор (например, Adam) для минимизации функции потерь. В процессе обучения веса модели подстраиваются таким образом, чтобы уменьшать расчетную потерю.

### 6. Оценка и итерации
После каждой эпохи обучения производится оценка качества модели на валидационном наборе данных. Если качество модели удовлетворительное, обучение прекращается, иначе процесс обучения продолжается с дополнительными итерациями.

## Установка

1. Клонируйте репозиторий:
```bash
   git clone https://github.com/AlexWortega/hr_search
   cd hr_search
```
2. Создайте и активируйте виртуальное окружение (опционально, но рекомендуется):
```bash
   python3 -m venv .env
   source .env/bin/activate
```
3. Установите все что вам надо
```bash
   pip install -r requirements.txt
```

3.1. Скачайте данные
Для работы с данными перейдите в директорию data
```bash
cd data
```

3.2. Перейдите в директорию подбора метрик, чтобы ознакомиться с результатами подбора модели и гиперпараметров
```bash
cd metrics
```

4. Запуск трейна
```bash
   python3 train.py --datapath data/train.pkl --modelname paraphrase-multilingual-mpnet-base-v2 --learningrate 1e-5 --batchsize 16 --margin 0.3 --epochs 10 --seed 42 --projectname paraphrase-training --checkpointpath paraphrasecheckpoint.pt --checkpointsteps 1000 --outputpath paraphrasemodel.pt
```


