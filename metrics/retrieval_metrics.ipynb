{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac093a53-7d2f-427a-9022-a6b6d443abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('../data/val.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5499a04-567d-414f-8ef0-ae10751a07d3",
   "metadata": {},
   "source": [
    "# Отбираю для теста retrival-ы c открытокого leaderboard https://github.com/avidale/encodechka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b973fa0-002f-48ea-8893-59a9cf31092d",
   "metadata": {},
   "source": [
    "пробуем прежде всего лидера, мультиязычная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85403bf8-0c76-468a-a7b3-97b25685fb83",
   "metadata": {},
   "source": [
    "я буду скачивать через git lfs, потому что у меня мало памяти. Можно напрямую вставлять c hf. Я заглядываю в лидерборт и копирую название в huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eeaf0df7-303f-4f9a-b358-efa28f6b27d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
      "Git LFS initialized.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning into 'multilingual-e5-large'...\n",
      "remote: Enumerating objects: 41, done.\u001b[K\n",
      "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 41 (delta 6), reused 3 (delta 3), pack-reused 31\u001b[K\n",
      "Unpacking objects: 100% (41/41), 51.61 KiB | 1.91 MiB/s, done.\n",
      "Filtering content: 100% (8/8), 6.29 GiB | 80.66 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/intfloat/multilingual-e5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc1f0706-850f-43e3-bd6c-0b4fb19fd46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "# Each input text should start with \"query: \" or \"passage: \", even for non-English texts.\n",
    "# For tasks other than retrieval, you can simply use the \"query: \" prefix.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('multilingual-e5-large')\n",
    "model = AutoModel.from_pretrained('multilingual-e5-large')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec71804-ca69-4f83-b750-b1e91a1910f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4509c4-c7e4-425e-a687-22c270a7a757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73d969090a44ce296404c6e5a89541e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Инициализируем список для хранения всех векторов предложений\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "all_sentence_embeddings1 = []\n",
    "batch_size = 50\n",
    "# Получаем количество сэмплов\n",
    "num_samples = len(df['position'].tolist())\n",
    "  \n",
    "# Проходим по всем сэмплам по batch_size\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    # Выбираем текущий батч\n",
    "    sentence_batch = df['position'].tolist()[i:i+batch_size]\n",
    "\n",
    "    # Tokenize sentences \n",
    "    batch_dict = tokenizer(sentence_batch, max_length=215, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    \n",
    "    outputs = model(**batch_dict)\n",
    "    sentence_embeddings1 = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    sentence_embeddings1 = F.normalize(sentence_embeddings1, p=2, dim=1)\n",
    "\n",
    "    # Добавляем вектора предложений текущего батча в список\n",
    "    all_sentence_embeddings1.append(sentence_embeddings1.cpu().detach().numpy())\n",
    "    del sentence_embeddings1\n",
    "    cleanup()\n",
    "    \n",
    "\n",
    "# Конкатенируем все вектора предложений\n",
    "sentence_embeddings1 = np.concatenate(all_sentence_embeddings1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7449c9c-276f-4394-9dbc-09d04a1a2262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a5d18b012a4f988e807976b0a1a873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#pas@5\n",
    "cum_sum_1 = 0\n",
    "RR = 0\n",
    "for index in tqdm(df.index):\n",
    "    question = df['description'][index]\n",
    "    \n",
    "    # это лайфхак для повторений топиков, я хочу, чтобы если модель найдет не свою строку, но такойже топик\n",
    "    # мы считали ее ответ за правильный в метрике\n",
    "    mask = df['position'] == df['position'][index]\n",
    "\n",
    "    new_df = df.copy()\n",
    "    new_df.index = df.index.where(~mask, index)\n",
    "\n",
    "    encoded_question = tokenizer(question, max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    outputs = model(**encoded_question)\n",
    "    embeddings1 = average_pool(outputs.last_hidden_state, encoded_question['attention_mask'])\n",
    "    embeddings1 = F.normalize(embeddings1, p=2, dim=1)\n",
    "    cos_similarities = cosine_similarity(embeddings1.cpu().detach().numpy(), sentence_embeddings1)[0]\n",
    "    new_df[\"rank\"]= cos_similarities\n",
    "    rank_s = new_df[\"rank\"].sort_values(ascending=False)\n",
    "    if index in rank_s[:1].index:\n",
    "        cum_sum_1 += 1\n",
    "    RR += 1/(list(rank_s.index).index(index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff929d88-33c5-45cf-b067-5572b8eb0827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат работы p@1: 2 %\n",
      "Mean reciprocal rank: 0.06\n"
     ]
    }
   ],
   "source": [
    "print(f\"Результат работы p@1: {int(100*cum_sum_1/len(df))} %\")\n",
    "print(f\"Mean reciprocal rank: {round((1/len(df))*RR,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6ed70-4e72-4049-a7b6-47fa6b7cfb5c",
   "metadata": {},
   "source": [
    "# Это было первое место в лидерборде, теперь попробуем некоторые модели ниже, у них у всех общий инференс, поэтому я буду скачивать, и когда буду пробовать новую, я просто буду менять имя вновь загруженной модели. То есть из таблицы этто люое имя, кроме уже ранее задействованного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ba5523-07ba-4189-b834-13dbbc820625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('../data/val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8940f10-b10d-4154-a31d-c265034ea125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
      "Git LFS initialized.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning into 'LaBSE'...\n",
      "remote: Enumerating objects: 54, done.\u001b[K\n",
      "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
      "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
      "remote: Total 54 (delta 16), reused 54 (delta 16), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (54/54), 7.12 MiB | 9.21 MiB/s, done.\n",
      "Filtering content: 100% (4/4), 5.26 GiB | 77.30 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/sentence-transformers/LaBSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718a8108-0a1a-4124-b8ec-fd9a3f785adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('paraphrase_documents_v3')\n",
    "model = AutoModel.from_pretrained('paraphrase_documents_v3')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7208bf74-e615-4669-be1b-d8dc6fae94a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6760fb9f8e5a4ef6abd0cd76b065ca3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Инициализируем список для хранения всех векторов предложений\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "all_sentence_embeddings1 = []\n",
    "batch_size = 100\n",
    "# Получаем количество сэмплов\n",
    "num_samples = len(df['position'].tolist())\n",
    "  \n",
    "# Проходим по всем сэмплам по batch_size\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    # Выбираем текущий батч\n",
    "    sentence_batch = df['position'].tolist()[i:i+batch_size]\n",
    "\n",
    "    # Tokenize sentences \n",
    "    encoded_input = tokenizer(sentence_batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Perform pooling. In this case, max pooling.\n",
    "    sentence_embeddings1 = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Добавляем вектора предложений текущего батча в список\n",
    "    all_sentence_embeddings1.append(sentence_embeddings1)\n",
    "\n",
    "# Конкатенируем все вектора предложений\n",
    "sentence_embeddings1 = torch.cat(all_sentence_embeddings1, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d9469ba-6ae4-4a2b-9a99-84c24c899ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a74de2913a48568a0d3cbab9c779f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат работы p@1: 2 %\n",
      "Mean reciprocal rank: 0.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#pas@5\n",
    "cum_sum_1 = 0\n",
    "RR = 0\n",
    "for index in tqdm(df.index):\n",
    "    question = df['description'][index]\n",
    "     # это лайфхак для повторений топиков, я хочу, чтобы если модель найдет не свою строку, но такойже топик\n",
    "    # мы считали ее ответ за правильный в метрике\n",
    "    mask = df['position'] == df['position'][index]\n",
    "\n",
    "    new_df = df.copy()\n",
    "    new_df.index = df.index.where(~mask, index)\n",
    "\n",
    "    encoded_question = tokenizer(question, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_question)\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_question['attention_mask'])\n",
    "    cos_similarities = cosine_similarity(sentence_embeddings.detach().cpu().numpy(), sentence_embeddings1)[0]\n",
    "    new_df[\"rank\"]= cos_similarities\n",
    "    rank_s = new_df[\"rank\"].sort_values(ascending=False)\n",
    "    if index in rank_s[:1].index:\n",
    "        cum_sum_1 += 1\n",
    "    RR += 1/(list(rank_s.index).index(index)+1)\n",
    "\n",
    "print(f\"Результат работы p@1: {int(100*cum_sum_1/len(df))} %\")\n",
    "print(f\"Mean reciprocal rank: {round((1/len(df))*RR,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683ec96-ca3c-4297-a1b8-cf0e2741da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarities = util.cos_sim(sentence_embeddings, sentence_embeddings1)\n",
    "df[\"rank\"]= cos_similarities[0].detach().cpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
