{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa41435-6200-4ae2-9a9a-4b6523f81c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # в collab не надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac093a53-7d2f-427a-9022-a6b6d443abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('df_test.pkl')\n",
    "# df = df.loc[df['Q'] != 'S']\n",
    "\n",
    "# file_path = 'TechSupportGeneratedDataSet.xlsx'\n",
    "# df = pd.read_excel(file_path, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7fa3963-afb7-4cc3-917f-332ff9f086b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[['T', 'Q']]\n",
    "df = df[['title_sol', 'Code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255de294-826a-4d88-8253-482b77b1bfd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'T'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeleting documents\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGuidance\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'T'"
     ]
    }
   ],
   "source": [
    "df.loc[(df['T'] != 'Deleting documents') & (df['T'] != 'Guidance'), 'Q'] = 'S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0c921c0-3854-48b3-bae2-4f2cea464fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Modified Preorder Traversal | Easy Approach'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_sol'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca4b32b-5e46-49ab-ae6b-3fdfd7b7ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa9e00d-76e3-4067-b091-4adf56c28a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>Guidance</td>\n",
       "      <td>Where can I find guidance on getting started w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>Guidance</td>\n",
       "      <td>How could I access the information about the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>Guidance</td>\n",
       "      <td>I'm feeling a bit lost. Where can I find guida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Guidance</td>\n",
       "      <td>What's the best place to learn about the funda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Guidance</td>\n",
       "      <td>Why can't I find the link to the guidance I need?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Deleting documents</td>\n",
       "      <td>Hello, I'm not sure how to check if a document...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Deleting documents</td>\n",
       "      <td>Hi, I'm trying to delete a folder, but it keep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Deleting documents</td>\n",
       "      <td>Hello, I'm a bit lost. How do I delete a docum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Deleting documents</td>\n",
       "      <td>Hi, I'm having trouble deleting files. Why doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Deleting documents</td>\n",
       "      <td>Hello, I'm trying to delete a document, but it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      T                                                  Q\n",
       "336            Guidance  Where can I find guidance on getting started w...\n",
       "337            Guidance  How could I access the information about the s...\n",
       "338            Guidance  I'm feeling a bit lost. Where can I find guida...\n",
       "339            Guidance  What's the best place to learn about the funda...\n",
       "340            Guidance  Why can't I find the link to the guidance I need?\n",
       "..                  ...                                                ...\n",
       "125  Deleting documents  Hello, I'm not sure how to check if a document...\n",
       "126  Deleting documents  Hi, I'm trying to delete a folder, but it keep...\n",
       "127  Deleting documents  Hello, I'm a bit lost. How do I delete a docum...\n",
       "128  Deleting documents  Hi, I'm having trouble deleting files. Why doe...\n",
       "129  Deleting documents  Hello, I'm trying to delete a document, but it...\n",
       "\n",
       "[190 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5499a04-567d-414f-8ef0-ae10751a07d3",
   "metadata": {},
   "source": [
    "# Отбираю для теста retrival-ы c открытокого leaderboard https://github.com/avidale/encodechka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b973fa0-002f-48ea-8893-59a9cf31092d",
   "metadata": {},
   "source": [
    "пробуем прежде всего лидера, мультиязычная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85403bf8-0c76-468a-a7b3-97b25685fb83",
   "metadata": {},
   "source": [
    "я буду скачивать через git lfs, потому что у меня мало памяти. Можно напрямую вставлять c hf. Я заглядываю в лидерборт и копирую название в huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eeaf0df7-303f-4f9a-b358-efa28f6b27d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
      "Git LFS initialized.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning into 'multilingual-e5-large'...\n",
      "remote: Enumerating objects: 41, done.\u001b[K\n",
      "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 41 (delta 6), reused 3 (delta 3), pack-reused 31\u001b[K\n",
      "Unpacking objects: 100% (41/41), 51.61 KiB | 1.91 MiB/s, done.\n",
      "Filtering content: 100% (8/8), 6.29 GiB | 80.66 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/intfloat/multilingual-e5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc1f0706-850f-43e3-bd6c-0b4fb19fd46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "# Each input text should start with \"query: \" or \"passage: \", even for non-English texts.\n",
    "# For tasks other than retrieval, you can simply use the \"query: \" prefix.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('paraphrase_code')\n",
    "model = AutoModel.from_pretrained('paraphrase_code')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec71804-ca69-4f83-b750-b1e91a1910f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4509c4-c7e4-425e-a687-22c270a7a757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73d969090a44ce296404c6e5a89541e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Инициализируем список для хранения всех векторов предложений\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "all_sentence_embeddings1 = []\n",
    "batch_size = 50\n",
    "# Получаем количество сэмплов\n",
    "num_samples = len(df['Code'].tolist())\n",
    "  \n",
    "# Проходим по всем сэмплам по batch_size\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    # Выбираем текущий батч\n",
    "    sentence_batch = df['Code'].tolist()[i:i+batch_size]\n",
    "\n",
    "    # Tokenize sentences \n",
    "    batch_dict = tokenizer(sentence_batch, max_length=215, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    \n",
    "    outputs = model(**batch_dict)\n",
    "    sentence_embeddings1 = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    sentence_embeddings1 = F.normalize(sentence_embeddings1, p=2, dim=1)\n",
    "\n",
    "    # Добавляем вектора предложений текущего батча в список\n",
    "    all_sentence_embeddings1.append(sentence_embeddings1.cpu().detach().numpy())\n",
    "    del sentence_embeddings1\n",
    "    cleanup()\n",
    "    \n",
    "\n",
    "# Конкатенируем все вектора предложений\n",
    "sentence_embeddings1 = np.concatenate(all_sentence_embeddings1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b2c40c-5a9a-4909-a52a-83de216a22f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence_embeddings1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msentence_embeddings1\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence_embeddings1' is not defined"
     ]
    }
   ],
   "source": [
    "sentence_embeddings1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7449c9c-276f-4394-9dbc-09d04a1a2262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a5d18b012a4f988e807976b0a1a873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#pas@5\n",
    "cum_sum_1 = 0\n",
    "RR = 0\n",
    "for index in tqdm(df.index):\n",
    "    question = df['title_sol'][index]\n",
    "    \n",
    "    # это лайфхак для повторений топиков, я хочу, чтобы если модель найдет не свою строку, но такойже топик\n",
    "    # мы считали ее ответ за правильный в метрике\n",
    "    mask = df['title_sol'] == df['title_sol'][index]\n",
    "\n",
    "    new_df = df.copy()\n",
    "    new_df.index = df.index.where(~mask, index)\n",
    "\n",
    "    encoded_question = tokenizer(question, max_length=512, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    outputs = model(**encoded_question)\n",
    "    embeddings1 = average_pool(outputs.last_hidden_state, encoded_question['attention_mask'])\n",
    "    embeddings1 = F.normalize(embeddings1, p=2, dim=1)\n",
    "    cos_similarities = cosine_similarity(embeddings1.cpu().detach().numpy(), sentence_embeddings1)[0]\n",
    "    new_df[\"rank\"]= cos_similarities\n",
    "    rank_s = new_df[\"rank\"].sort_values(ascending=False)\n",
    "    if index in rank_s[:1].index:\n",
    "        cum_sum_1 += 1\n",
    "    RR += 1/(list(rank_s.index).index(index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff929d88-33c5-45cf-b067-5572b8eb0827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат работы p@1: 2 %\n",
      "Mean reciprocal rank: 0.06\n"
     ]
    }
   ],
   "source": [
    "print(f\"Результат работы p@1: {int(100*cum_sum_1/len(df))} %\")\n",
    "print(f\"Mean reciprocal rank: {round((1/len(df))*RR,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6ed70-4e72-4049-a7b6-47fa6b7cfb5c",
   "metadata": {},
   "source": [
    "# Это было первое место в лидерборде, теперь попробуем некоторые модели ниже, у них у всех общий инференс, поэтому я буду скачивать, и когда буду пробовать новую, я просто буду менять имя вновь загруженной модели. То есть из таблицы этто люое имя, кроме уже ранее задействованного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ba5523-07ba-4189-b834-13dbbc820625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = 'TechSupportGeneratedDataSet.xlsx'\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "df = df[['T', 'Q']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8940f10-b10d-4154-a31d-c265034ea125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
      "Git LFS initialized.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning into 'LaBSE'...\n",
      "remote: Enumerating objects: 54, done.\u001b[K\n",
      "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
      "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
      "remote: Total 54 (delta 16), reused 54 (delta 16), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (54/54), 7.12 MiB | 9.21 MiB/s, done.\n",
      "Filtering content: 100% (4/4), 5.26 GiB | 77.30 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/sentence-transformers/LaBSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718a8108-0a1a-4124-b8ec-fd9a3f785adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('paraphrase_documents_v3')\n",
    "model = AutoModel.from_pretrained('paraphrase_documents_v3')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7208bf74-e615-4669-be1b-d8dc6fae94a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6760fb9f8e5a4ef6abd0cd76b065ca3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Инициализируем список для хранения всех векторов предложений\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "all_sentence_embeddings1 = []\n",
    "batch_size = 100\n",
    "# Получаем количество сэмплов\n",
    "num_samples = len(df['Code'].tolist())\n",
    "  \n",
    "# Проходим по всем сэмплам по batch_size\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    # Выбираем текущий батч\n",
    "    sentence_batch = df['Code'].tolist()[i:i+batch_size]\n",
    "\n",
    "    # Tokenize sentences \n",
    "    encoded_input = tokenizer(sentence_batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Perform pooling. In this case, max pooling.\n",
    "    sentence_embeddings1 = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Добавляем вектора предложений текущего батча в список\n",
    "    all_sentence_embeddings1.append(sentence_embeddings1)\n",
    "\n",
    "# Конкатенируем все вектора предложений\n",
    "sentence_embeddings1 = torch.cat(all_sentence_embeddings1, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d9469ba-6ae4-4a2b-9a99-84c24c899ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a74de2913a48568a0d3cbab9c779f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат работы p@1: 2 %\n",
      "Mean reciprocal rank: 0.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#pas@5\n",
    "cum_sum_1 = 0\n",
    "RR = 0\n",
    "for index in tqdm(df.index):\n",
    "    question = df['title_sol'][index]\n",
    "    if question != 'S':\n",
    "     # это лайфхак для повторений топиков, я хочу, чтобы если модель найдет не свою строку, но такойже топик\n",
    "    # мы считали ее ответ за правильный в метрике\n",
    "        mask = df['Code'] == df['Code'][index]\n",
    "\n",
    "        new_df = df.copy()\n",
    "        new_df.index = df.index.where(~mask, index)\n",
    "\n",
    "        encoded_question = tokenizer(question, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_question)\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_question['attention_mask'])\n",
    "        cos_similarities = cosine_similarity(sentence_embeddings.detach().cpu().numpy(), sentence_embeddings1)[0]\n",
    "        new_df[\"rank\"]= cos_similarities\n",
    "        rank_s = new_df[\"rank\"].sort_values(ascending=False)\n",
    "        if index in rank_s[:1].index:\n",
    "            cum_sum_1 += 1\n",
    "        RR += 1/(list(rank_s.index).index(index)+1)\n",
    "\n",
    "print(f\"Результат работы p@1: {int(100*cum_sum_1/len(df))} %\")\n",
    "print(f\"Mean reciprocal rank: {round((1/len(df))*RR,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35bb2922-017a-4f97-84e2-cc5476fd81b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "915"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cda9ca-d1ff-4aef-8a7e-4c78d8e40038",
   "metadata": {},
   "source": [
    "# Продемонстрирую использование лучшей модели paraphrase-multilingual-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f1d0cd-efb0-48c5-be1d-9fe2b689acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5449fb-c2e6-493d-8cfc-9aa449bee30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
      "Git LFS initialized.\n",
      "Cloning into 'paraphrase-multilingual-mpnet-base-v2'...\n",
      "remote: Enumerating objects: 28, done.\u001b[K\n",
      "remote: Total 28 (delta 0), reused 0 (delta 0), pack-reused 28\u001b[K\n",
      "Unpacking objects: 100% (28/28), 3.38 MiB | 4.91 MiB/s, done.\n",
      "Filtering content: 100% (3/3), 2.07 GiB | 46.10 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4992a92c-9271-4f90-aec4-ea050fcd7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'TechSupportGeneratedDataSet.xlsx'\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "df = df[['T', 'Q']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee83621-5ef6-4ad0-9ac0-e221d82a0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['T'])\n",
    "df = df.reset_index() \n",
    "df.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a16d281c-6502-4cfa-af83-a4f8c956fad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deleting documents</td>\n",
       "      <td>Hey there, I've been trying to delete multiple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Releasing</td>\n",
       "      <td>Hey, I'm having trouble releasing a document. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creating new files</td>\n",
       "      <td>How do I create a new file in the system?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guidance</td>\n",
       "      <td>Where can I find guidance on getting started w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Privileges</td>\n",
       "      <td>I'm having trouble accessing a document becaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Moving</td>\n",
       "      <td>I'm trying to move a couple of folders, but I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Creating a folder</td>\n",
       "      <td>Hello, I need to create a new folder to organi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Creating physical files</td>\n",
       "      <td>I'm not sure how to create a physical file in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Creating virtual files</td>\n",
       "      <td>How can I differentiate between a virtual file...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         T                                                  Q\n",
       "0       Deleting documents  Hey there, I've been trying to delete multiple...\n",
       "1                Releasing  Hey, I'm having trouble releasing a document. ...\n",
       "2       Creating new files          How do I create a new file in the system?\n",
       "3                 Guidance  Where can I find guidance on getting started w...\n",
       "4               Privileges  I'm having trouble accessing a document becaus...\n",
       "5                   Moving  I'm trying to move a couple of folders, but I'...\n",
       "6        Creating a folder  Hello, I need to create a new folder to organi...\n",
       "7  Creating physical files  I'm not sure how to create a physical file in ...\n",
       "8   Creating virtual files  How can I differentiate between a virtual file..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14d0f0c9-a73f-4de4-b3df-19f62906022f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('paraphrase_code')\n",
    "model = AutoModel.from_pretrained('paraphrase_code')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d28055cd-3bd8-45a4-8a9d-8d66cd55443f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f6233cf52e42dbbf324481d8e47a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Инициализируем список для хранения всех векторов предложений\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "all_sentence_embeddings1 = []\n",
    "batch_size = 100\n",
    "# Получаем количество сэмплов\n",
    "num_samples = len(df['Code'].tolist())\n",
    "  \n",
    "# Проходим по всем сэмплам по batch_size\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    # Выбираем текущий батч\n",
    "    sentence_batch = df['Code'].tolist()[i:i+batch_size]\n",
    "\n",
    "    # Tokenize sentences \n",
    "    encoded_input = tokenizer(sentence_batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Perform pooling. In this case, max pooling.\n",
    "    sentence_embeddings1 = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Добавляем вектора предложений текущего батча в список\n",
    "    all_sentence_embeddings1.append(sentence_embeddings1.cpu().detach().numpy())\n",
    "    del sentence_embeddings1\n",
    "    cleanup()\n",
    "    \n",
    "# Конкатенируем все вектора предложений\n",
    "sentence_embeddings1 = np.concatenate(all_sentence_embeddings1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a1840-8db6-4923-a772-93b2c5859e18",
   "metadata": {},
   "source": [
    "пока это для примера, без ускорений и тд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f09f1f90-af10-4ae4-9b38-3cae2ceb1342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /home/zjkarina/.local/lib/python3.8/site-packages (from sentence_transformers) (0.16.4)\n",
      "Requirement already satisfied: nltk in /home/zjkarina/.local/lib/python3.8/site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.24.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: tqdm in /home/zjkarina/.local/lib/python3.8/site-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/zjkarina/.local/lib/python3.8/site-packages (from sentence_transformers) (4.31.0)\n",
      "Requirement already satisfied: filelock in /home/zjkarina/.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /home/zjkarina/.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /home/zjkarina/.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (22.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/zjkarina/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (45.2.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (0.34.2)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.27.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zjkarina/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/zjkarina/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.1)\n",
      "Requirement already satisfied: click in /home/zjkarina/.local/lib/python3.8/site-packages (from nltk->sentence_transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence_transformers) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zjkarina/.local/lib/python3.8/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zjkarina/.local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zjkarina/.local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2019.11.28)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125922 sha256=e1fea72fb4aaafb7efc1f23663d13e09958d93afc58ffcdd9c6e406a43ecbc7a\n",
      "  Stored in directory: /home/zjkarina/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
      "Successfully built sentence_transformers\n",
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de0ae0f6-ed2f-47e3-8ae7-99399af646d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: ```\n",
      "# Definition for a binary tree node.\n",
      "# class TreeNode:\n",
      "#     def __init__(self, val=0, left=None, right=None):\n",
      "#          = val\n",
      "#          = left\n",
      "#          = right\n",
      "class Solution:\n",
      "    def __init__(self):\n",
      "         = []\n",
      "\n",
      "    def preorderTraversal(self, root: Optional[TreeNode]) -> List[int]:\n",
      "        (root)\n",
      "        return \n",
      "\n",
      "    def preorder(self, root):\n",
      "        if root is None:\n",
      "            return\n",
      "        ()\n",
      "        ()\n",
      "        ()\n",
      "```\n",
      "Similiarity: 0.6930761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "question = \"Modified Preorder Traversal | Easy Approach\" #Введи тут вопрос\n",
    "\n",
    "encoded_question = tokenizer(question, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_question)\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_question['attention_mask'])\n",
    "cos_similarities = cosine_similarity(sentence_embeddings.detach().cpu().numpy(), sentence_embeddings1)[0]\n",
    "df[\"rank\"]= cos_similarities\n",
    "rank_s = df[\"rank\"].sort_values(ascending=False)\n",
    "print('Topic:', df['Code'][rank_s[:1].index.item()])\n",
    "print('Similiarity:', rank_s.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136050aa-bbe2-4502-9e79-131cfe4148b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#pas@5\n",
    "cum_sum_1 = 0\n",
    "RR = 0\n",
    "for index in tqdm(df.index):\n",
    "    question = df[''][index]\n",
    "    if question != 'S':\n",
    "     # это лайфхак для повторений топиков, я хочу, чтобы если модель найдет не свою строку, но такойже топик\n",
    "    # мы считали ее ответ за правильный в метрике\n",
    "        mask = df['T'] == df['T'][index]\n",
    "\n",
    "        new_df = df.copy()\n",
    "        new_df.index = df.index.where(~mask, index)\n",
    "\n",
    "        encoded_question = tokenizer(question, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_question)\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_question['attention_mask'])\n",
    "        cos_similarities = cosine_similarity(sentence_embeddings.detach().cpu().numpy(), sentence_embeddings1.detach().cpu().numpy())[0]\n",
    "        new_df[\"rank\"]= cos_similarities\n",
    "        rank_s = new_df[\"rank\"].sort_values(ascending=False)\n",
    "        if index in rank_s[:1].index:\n",
    "            cum_sum_1 += 1\n",
    "        RR += 1/(list(rank_s.index).index(index)+1)\n",
    "\n",
    "print(f\"Результат работы p@1: {int(100*cum_sum_1/len(df))} %\")\n",
    "print(f\"Mean reciprocal rank: {round((1/len(df))*RR,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cced31fc-e00e-4a56-a314-19074c1c631c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0287, -0.0384,  0.4020, -0.0283, -0.0494, -0.0069,  0.1553,  0.8246,\n",
       "        -0.0137], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683ec96-ca3c-4297-a1b8-cf0e2741da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarities = util.cos_sim(sentence_embeddings, sentence_embeddings1)\n",
    "df[\"rank\"]= cos_similarities[0].detach().cpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
